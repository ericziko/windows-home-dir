# quantized model is used in this config.
ModelType=ONNX_TRANSFORMER_ENCODER
ModelEncoder=encoder.onnx
ModelPredictor=decoder.onnx
ModelJoint=joint.onnx
TransformerEncoderMaxChunks=8
ModelLID=lid_quantized.onnx
